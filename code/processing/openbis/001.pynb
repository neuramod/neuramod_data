{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa01eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session is no longer valid. Please log in again.\n"
     ]
    }
   ],
   "source": [
    "# OPENBIS\n",
    "from pybis import Openbis\n",
    "o = Openbis()\n",
    "# Use this code to reconnect in case your openBIS session expires and you an error on the previous step.\n",
    "sessionToken = \"pierrecu-230125153324455xA4B4F65B8FD0B2BCEAD19A977F73F1E7\"\n",
    "o.set_token(token=sessionToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus-central.leomed.ethz.ch/repository/pypi/simple\n",
      "Requirement already satisfied: mne in /vagrant_installation/miniconda3/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (1.7.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (1.6.0)\n",
      "Requirement already satisfied: tqdm in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (4.62.3)\n",
      "Requirement already satisfied: matplotlib in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (1.21.4)\n",
      "Requirement already satisfied: packaging in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (21.3)\n",
      "Requirement already satisfied: jinja2 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (3.0.3)\n",
      "Requirement already satisfied: decorator in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne) (5.1.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from pooch>=1.5->mne) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from pooch>=1.5->mne) (2.26.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from packaging->mne) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from jinja2->mne) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (4.28.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne) (6.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mne) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib->mne) (59.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib->mne) (1.2.2)\n",
      "Looking in indexes: https://nexus-central.leomed.ethz.ch/repository/pypi/simple\n",
      "Requirement already satisfied: mne_bids in /vagrant_installation/miniconda3/lib/python3.9/site-packages (0.12)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne_bids) (1.21.4)\n",
      "Requirement already satisfied: setuptools in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne_bids) (59.2.0)\n",
      "Requirement already satisfied: mne>=1.2 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne_bids) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne_bids) (1.7.2)\n",
      "Requirement already satisfied: jinja2 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (3.0.3)\n",
      "Requirement already satisfied: tqdm in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (4.62.3)\n",
      "Requirement already satisfied: pooch>=1.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (1.6.0)\n",
      "Requirement already satisfied: matplotlib in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (3.5.0)\n",
      "Requirement already satisfied: packaging in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (21.3)\n",
      "Requirement already satisfied: decorator in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from mne>=1.2->mne_bids) (5.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from pooch>=1.5->mne>=1.2->mne_bids) (2.26.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from pooch>=1.5->mne>=1.2->mne_bids) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from packaging->mne>=1.2->mne_bids) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from jinja2->mne>=1.2->mne_bids) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (4.28.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (8.4.0)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (6.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from matplotlib->mne>=1.2->mne_bids) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mne>=1.2->mne_bids) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.2->mne_bids) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.2->mne_bids) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.2->mne_bids) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.2->mne_bids) (3.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib->mne>=1.2->mne_bids) (1.2.2)\n",
      "Looking in indexes: https://nexus-central.leomed.ethz.ch/repository/pypi/simple\n",
      "Requirement already satisfied: pybv==0.6.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from pybv==0.6.0) (1.21.4)\n",
      "Looking in indexes: https://nexus-central.leomed.ethz.ch/repository/pypi/simple\r\n",
      "Requirement already satisfied: openpyxl in /vagrant_installation/miniconda3/lib/python3.9/site-packages (3.0.10)\r\n",
      "Requirement already satisfied: et-xmlfile in /vagrant_installation/miniconda3/lib/python3.9/site-packages (from openpyxl) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "# INSTALL ADDITIONAL PACKAGES\n",
    "!pip install --index-url https://nexus-central.leomed.ethz.ch/repository/pypi/simple mne\n",
    "!pip install --index-url https://nexus-central.leomed.ethz.ch/repository/pypi/simple mne_bids\n",
    "!pip install --index-url https://nexus-central.leomed.ethz.ch/repository/pypi/simple pybv==0.6.0\n",
    "!pip install --index-url https://nexus-central.leomed.ethz.ch/repository/pypi/simple openpyxl\n",
    "    \n",
    "# INFOS\n",
    "# RESTART THE SERVER IF THE FOLLOWING ERROR OCCURS\n",
    "#    ValueError: events: when `type` is Stimulus, descriptions must be positve ints.\n",
    "#OR\n",
    "#    IndexError: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIBRARIES\n",
    "# BUILT-IN\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import copy\n",
    "\n",
    "# THIRD PART\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import pybv\n",
    "import mne_bids\n",
    "\n",
    "from mne_bids import mark_channels\n",
    "from mne_bids import write_raw_bids, BIDSPath, print_dir_tree, make_dataset_description, update_sidecar_json\n",
    "from mne_bids.stats import count_events\n",
    "\n",
    "# CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508abf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PARAMS\n",
    "\n",
    "raw_modes = ['BLED','LSLD'] # these recording modes will transformed into BIDS datasets\n",
    "bids_path = './BIDS' # local path for temporary downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHODS\n",
    "def event_sequence(rest_time, stimulus_display, stimuli_number, trial_rest, trial_length, sampling_frequency, data = []):\n",
    "    \"\"\"Return a sequence of events from a stimulation file.\"\"\"\n",
    "    \n",
    "    # DEBUG\n",
    "    #for i in range(event_start,trial_length + 1, math.floor(sampling_frequency/2)):\n",
    "    #\n",
    "    for i in range(trial_rest,trial_length + 1, int(sampling_frequency*stimulus_display)):\n",
    "        data.append(i)\n",
    "        \n",
    "    sequence = pd.DataFrame(data)\n",
    "    m = sequence.iloc[-1,:]\n",
    "    next_trial_rest = int((rest_time * sampling_frequency) + m)\n",
    "    next_trial_length = int(next_trial_rest + (stimulus_display * sampling_frequency * stimuli_number))\n",
    "    \n",
    "    return data, next_trial_rest, next_trial_length\n",
    "#\n",
    "def createBaseBidsFile(eeg_file, layout_file, events_file, base_path = '.', unit = 'µV', sampling_frequency = 256, standard_montage = 'standard_1020', authors = ['',], funding = ''):\n",
    "    \"\"\"Create the base structure and files for Bids standards without additional infos.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(eeg_file)\n",
    "    layout_raw = pd.read_excel(layout_file,index_col = 0)\n",
    "    df_mne = df.drop(['timestamp', 'sequence', 'battery', 'flags'], axis=1)# based on bitbrain file format\n",
    "    # change after Phase 1\n",
    "    df_mne[\"EEG\"]=df_mne['EEG_ch1']\n",
    "    #\n",
    "    ch_names = []\n",
    "    \n",
    "    eeg_cols = [col for col in df.columns if 'EEG' in col]\n",
    "\n",
    "    for i in range(len(eeg_cols)):\n",
    "        the_name = layout_raw.index[layout_raw['Channel number'] == i+1].tolist()[0].replace('0','O')\n",
    "        ch_names.append(the_name)\n",
    "\n",
    "    data = df_mne.to_numpy().transpose()\n",
    "    data = np.vstack(data/10e5) if unit == 'µV' else None\n",
    "    #unit = [unit] * len(eeg_cols)\n",
    "    \n",
    "    event = pd.read_csv(events_file)\n",
    "    events = np.asarray(event)\n",
    "\n",
    "    code = f\"{events_dict['participant_id']}_{events_dict['project_phase']}_{events_dict['session_id']}_{events_dict['task_name']}\"\n",
    "    pybv.write_brainvision(data = data, \n",
    "                           sfreq = float(sampling_frequency), \n",
    "                           ch_names = ch_names,\n",
    "                           folder_out = os.path.join(base_path,f\"{code}\"),\n",
    "                           fname_base = code, \n",
    "                           events = events,\n",
    "                           unit = unit,\n",
    "                           overwrite = True)\n",
    "    # change after Phase 1\n",
    "    #raw = mne.io.read_raw_brainvision(os.path.join(base_path,code,f\"{code}.vhdr\"), preload=False)\n",
    "    raw = mne.io.read_raw_brainvision(os.path.join(base_path,code,f\"{code}.vhdr\"), preload=False)\n",
    "    raw.set_channel_types(mapping={'EOG': 'eeg'})\n",
    "    raw.rename_channels(mapping={'EOG': 'EOG'})\n",
    "    raw.set_channel_types(mapping={'EOG': 'eog'})\n",
    "    #\n",
    "    montage = mne.channels.make_standard_montage(standard_montage)\n",
    "    raw = raw.copy().set_montage(montage)\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    print(raw.annotations)\n",
    "    print(write_raw_bids.__doc__)\n",
    "    \n",
    "    bids_path = BIDSPath(subject = events_dict['participant_id'],\n",
    "                         task = events_dict['task_name'],\n",
    "                         root = os.path.join(base_path,code,code)\n",
    "                         )\n",
    "    write_raw_bids(raw, bids_path, overwrite = True)\n",
    "    make_dataset_description(os.path.join(base_path,code,code), \n",
    "                             code, \n",
    "                             authors = authors, \n",
    "                             funding = funding, \n",
    "                             overwrite = True)\n",
    "    \n",
    "    return os.path.join(base_path,f\"{code}\")\n",
    "#\n",
    "def updateBidsInfos(subject = None, task = None, suffix = None, datatype = None, root = None, extension = '.json', entries_dict = {}):\n",
    "    \"\"\"Add additional informations to a bids data folder.\"\"\"\n",
    "    \n",
    "    bids_path = BIDSPath(subject = subject,\n",
    "                         task = task, \n",
    "                         suffix = suffix,\n",
    "                         datatype = datatype, \n",
    "                         root = root\n",
    "                        )                                         \n",
    "    sidecar_path = bids_path.copy().update(extension = extension)\n",
    "    entries = entries_dict\n",
    "    update_sidecar_json(bids_path = sidecar_path, \n",
    "                        entries = entries)\n",
    "#\n",
    "def updateBidsChannels(the_path, the_id, the_task, hi = 'n/a', lo = 'n/a', desc = 'EEG'):\n",
    "    \"\"\"Overwrite a Bids channel file with missing basic infos.\"\"\"\n",
    "    \n",
    "    fname = os.path.join(the_path, f\"sub-{the_id}\", f\"{desc.lower()}\", f\"sub-{the_id}_task-{the_task}_channels.tsv\")\n",
    "    df = pd.read_csv(fname, sep=\"\\t\")\n",
    "    df[\"low_cutoff\"] = lo\n",
    "    df[\"high_cutoff\"] = hi\n",
    "    df[\"description\"] = desc\n",
    "    df.to_csv(fname, \n",
    "              index = False, \n",
    "              sep = \"\\t\",\n",
    "              na_rep = \"n/a\")\n",
    "#\n",
    "def updateBidsParticipants(the_path, participant_dict):\n",
    "    \"\"\"Overwrite a Bids participants file with missing basic infos.\"\"\"\n",
    "    \n",
    "    fname = os.path.join(the_path, \"participants.tsv\")\n",
    "    df = pd.read_csv(fname, \n",
    "                     sep = \"\\t\")\n",
    "    for _k in participant_dict:\n",
    "        df[_k] = participant_dict[_k]\n",
    "    df.to_csv(fname, \n",
    "              index = False, \n",
    "              sep = \"\\t\", \n",
    "              na_rep = \"n/a\")\n",
    "    \n",
    "#\n",
    "def readSamplingFrequencyFromRawData(json_file, signal_name):\n",
    "    \"\"\"Read a Json File and return its indicated sampling frequency for a given signal name.\n",
    "       Based on BitBrain file formats.\n",
    "    \"\"\"\n",
    "    \n",
    "    sampling_frequency = None\n",
    "    \n",
    "    with open(json_file) as jf:\n",
    "        info = json.load(jf)\n",
    "        for _signal in info['signals']:\n",
    "            if signal_name in _signal['filename']:\n",
    "                sampling_frequency = int(_signal['sampling_rate'])\n",
    "    \n",
    "    return sampling_frequency\n",
    "#\n",
    "def createEvents(save_path, data_csv_file, old_label, trial_number, sampling_frequency, trial_start, stimulus_display, datapoints_start, rest_time, stimuli_number, participant_id, project_phase, session_id, task_name):\n",
    "    \"\"\"Create events file for a task from a stimulation file.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_csv_file)\n",
    "    df.rename(columns = {f'{old_label}':'event_label'}, inplace = True)\n",
    "    event_label = df['event_label']\n",
    "    event_label.dropna(inplace=True)\n",
    "    event_label = event_label.astype('int')\n",
    "    event_start = int(sampling_frequency * trial_start)\n",
    "    #trial_length = int(stimulus_display * sampling_frequency * datapoints_start)\n",
    "    #trial_rest = trial_length + int(rest_time * sampling_frequency)\n",
    "    trial_rest = event_start + int(rest_time * sampling_frequency)\n",
    "    trial_length = trial_rest + int(stimulus_display * sampling_frequency * datapoints_start)\n",
    "    events = event_label.to_frame()\n",
    "    x = []\n",
    "    \n",
    "    \n",
    "    for _t in range(trial_number):\n",
    "        x,trial_rest,trial_length = event_sequence(rest_time = rest_time, \n",
    "                                                    stimulus_display = stimulus_display, \n",
    "                                                    stimuli_number = stimuli_number, \n",
    "                                                    trial_rest = trial_rest, \n",
    "                                                    trial_length = trial_length, \n",
    "                                                    sampling_frequency = sampling_frequency, \n",
    "                                                    data = x)\n",
    "    # DEBUG\n",
    "    print(f\"DEBUG x len: {len(x)}\")\n",
    "    print(f\"DEBUG events size: {events.size}\")\n",
    "    print(f\"DEBUG trial_number: {trial_number}\")\n",
    "    print(f\"DEBUG datapoints_start: {datapoints_start}\")\n",
    "    print(f\"DEBUG rest_time: {rest_time}\")\n",
    "    print(f\"DEBUG stimulus_display: {stimulus_display}\")\n",
    "    print(f\"DEBUG stimuli_number: {stimuli_number}\")\n",
    "    print(f\"DEBUG event_start: {event_start}\")\n",
    "    print(f\"DEBUG trial_length: {trial_length}\")\n",
    "    print(f\"DEBUG trial_rest: {trial_rest}\")\n",
    "    print(f\"DEBUG sampling_frequency: {sampling_frequency}\")\n",
    "    #\n",
    "    \n",
    "    events['event_sequence'] = x\n",
    "    events = events[['event_sequence', 'event_label']]\n",
    "    events['trial'] = \"\"\n",
    "    events['trial'] = [i for i in range(1, trial_number + 1) for j in range(stimuli_number + 1)]\n",
    "    events.to_csv(os.path.join(save_path, f\"{participant_id}_{project_phase}_{session_id}_{task_name}_events.csv\"), index = False)\n",
    "#\n",
    "def createReadme(path,data_dict):\n",
    "    \"\"\"Create a readme file in the json format at given path and with given dict.\"\"\"\n",
    "    \n",
    "    readme_path = os.path.join(path,\"README.json\")\n",
    "    temp = {}\n",
    "    \n",
    "    with open(readme_path, \"w\") as outfile:\n",
    "        # simplify subkeys to their attribute name\n",
    "        for _k in data_dict:\n",
    "            temp[_k] = {}\n",
    "            for _subk in data_dict[_k]:\n",
    "                if '.' in _subk:\n",
    "                    temp[_k][_subk.split('.')[1]] = data_dict[_k][_subk]\n",
    "                else:\n",
    "                    temp[_k][_subk] = data_dict[_k][_subk]\n",
    "           \n",
    "        json.dump(temp, outfile, indent = 4)\n",
    "        #json.dump(data_dict, outfile)\n",
    "    \n",
    "    return readme_path\n",
    "#\n",
    "def createBidsFiles(events_dict,the_participant):\n",
    "    \"\"\"Create events, brainvision and bids files and return their list\"\"\"\n",
    "    \n",
    "    list_of_files = []\n",
    "    the_participant = o.get_object(the_participant)\n",
    "    \n",
    "    # DEBUG\n",
    "    print(f\"- stim.rep_time: {int(events_dict['stim_infos']['stim.rep_time'])}\")\n",
    "    print(f\"- stim.num: {int(events_dict['stim_infos']['stim.num'])}\")\n",
    "    print(f\"- stim.rest_time: {events_dict['stim_infos']['stim.rest_time']}\")\n",
    "    #\n",
    "    \n",
    "    # create an event file by task\n",
    "    createEvents(save_path = events_dict['save_path'], \n",
    "                 data_csv_file = events_dict['stim_csv_file'],\n",
    "                 \n",
    "                 #old_label = events_dict['old_label'],\n",
    "                 old_label = events_dict['stim_infos']['stim.label_name'],\n",
    "                 \n",
    "                 #trial_number = events_dict['trial_number'],\n",
    "                 trial_number = int(events_dict['stim_infos']['stim.trials_number']),\n",
    "                 \n",
    "                 #sampling_frequency = events_dict['sampling_frequency'],\n",
    "                 sampling_frequency = int(events_dict['acquisition_infos']['sensor.sampling_frequency']),\n",
    "                 \n",
    "                 #trial_start = events_dict['trial_start'],\n",
    "                 trial_start = float(events_dict['stim_infos']['stim.trial_start']),\n",
    "                 \n",
    "                 #stimulus_display = events_dict['stimulus_display'],\n",
    "                 stimulus_display = float(events_dict['stim_infos']['stim.display_time']) + float(events_dict['stim_infos']['stim.notdisplay_time']),\n",
    "                 \n",
    "                 #datapoints_start = events_dict['datapoints_start'],\n",
    "                 datapoints_start = int(events_dict['stim_infos']['stim.datapoints_start']),\n",
    "                 \n",
    "                 #rest_time = events_dict['rest_time'],\n",
    "                 rest_time = float(events_dict['stim_infos']['stim.rest_time']),\n",
    "                 \n",
    "                 #stimuli_number = events_dict['stimuli_number'],\n",
    "                 stimuli_number = int(events_dict['stim_infos']['stim.rep_time']) * int(events_dict['stim_infos']['stim.num']) - 1,\n",
    "                 \n",
    "                 participant_id = events_dict['participant_id'], \n",
    "                 project_phase = events_dict['project_phase'], \n",
    "                 session_id = events_dict['session_id'], \n",
    "                 \n",
    "                 #task_name = events_dict['task_name'],\n",
    "                 task_name = events_dict['stim_infos']['stim.name']\n",
    "                )\n",
    "    the_events_file = os.path.join(events_dict['save_path'], f\"{events_dict['participant_id']}_{events_dict['project_phase']}_{events_dict['session_id']}_{ events_dict['stim_infos']['stim.name']}_events.csv\")\n",
    "    list_of_files.append(the_events_file)\n",
    "    \n",
    "    # create a base bids file\n",
    "    bids_root = createBaseBidsFile(eeg_file = events_dict['raw_eeg_file'], \n",
    "                                   layout_file = events_dict['eeg_layout_file'],\n",
    "                                   events_file = the_events_file,\n",
    "                                   base_path = events_dict['save_path'], \n",
    "                                   unit = events_dict['acquisition_infos']['sensor.units'],\n",
    "                                   sampling_frequency = events_dict['acquisition_infos']['sensor.sampling_frequency'], \n",
    "                                   standard_montage = events_dict['acquisition_infos']['sensor.layout'],\n",
    "                                   authors = events_dict['project_infos']['proj.authors'].split(','), \n",
    "                                   funding = events_dict['project_infos']['proj.funding']\n",
    "                                  )\n",
    "    sub_path = f\"{events_dict['participant_id']}_{events_dict['project_phase']}_{events_dict['session_id']}_{ events_dict['stim_infos']['stim.name']}\"\n",
    "    sub_root = os.path.join(f\"{bids_root}\",sub_path)\n",
    "    \n",
    "    # update eeg info\n",
    "    updateBidsInfos(subject = events_dict['participant_id'], \n",
    "                    task = events_dict['stim_infos']['stim.name'], \n",
    "                    suffix = events_dict['acquisition_infos']['sensor.modality'].lower()[:-1], \n",
    "                    datatype = events_dict['acquisition_infos']['sensor.modality'].lower()[:-1], \n",
    "                    root = sub_root, \n",
    "                    extension = '.json', \n",
    "                    entries_dict = events_dict['acquisition_infos'],\n",
    "                   )\n",
    "    # update participant infos\n",
    "    \n",
    "    temp_participant = {}\n",
    "    for _k in the_participant.props():\n",
    "        if 'participant.id' in _k:\n",
    "            #temp_participant['participant_id'] = {the_participant.props()[_k],dict(\"Description\":\"\",\"Levels\":\"\")}\n",
    "            temp_participant['participant_id'] = {the_participant.props()[_k],{\"Description\":\"\",\"Levels\":\"\"}}\n",
    "        elif '.' in _k:\n",
    "            temp_participant[_k.split('.')[1]] = {the_participant.props()[_k],{\"Description\":\"\",\"Levels\":\"\"}}\n",
    "        else:\n",
    "            temp_participant[_k] = {the_participant.props()[_k],{\"Description\":\"\",\"Levels\":\"\"}}\n",
    "        \n",
    "    \n",
    "    updateBidsInfos(subject = None, \n",
    "                    task = None, \n",
    "                    suffix = 'participants', \n",
    "                    datatype = None, \n",
    "                    root = sub_root, \n",
    "                    extension = '.json', \n",
    "                    entries_dict = temp_participant, #the_participant.props(),\n",
    "                   )\n",
    "    #update channels infos\n",
    "    updateBidsChannels(the_path = sub_root,\n",
    "                       the_id = events_dict['participant_id'], \n",
    "                       the_task = events_dict['stim_infos']['stim.name'], \n",
    "                       hi = events_dict['acquisition_infos']['sensor.high_pass'], \n",
    "                       lo = events_dict['acquisition_infos']['sensor.low_pass'], \n",
    "                       desc = events_dict['acquisition_infos']['sensor.modality'].lower()[:-1]\n",
    "                      )\n",
    "    #update participants infos\n",
    "    updateBidsParticipants(the_path = sub_root,\n",
    "                           participant_dict = the_participant.props()\n",
    "                          )\n",
    "    #create a general description readme file\n",
    "    createReadme(sub_root,{'project':events_dict['project_infos'],\n",
    "                            'sensors':events_dict['acquisition_infos'],\n",
    "                            'stimulation':events_dict['stim_infos']})\n",
    "                               \n",
    "    #zip bids folder\n",
    "    #zip_path = os.path.join(sub_root,sub_path)\n",
    "    zip_path = sub_root\n",
    "    \n",
    "    with ZipFile(f\"{zip_path}.zip\", mode='w') as zipf:\n",
    "        len_dir_path = len(zip_path)\n",
    "        for root, _, files in os.walk(zip_path):\n",
    "            for file in files:\n",
    "                if \".zip\" not in file:\n",
    "                    # rename old Readme file with Bids authors to a reference file\n",
    "                    if 'README' in file and '.json' not in file:\n",
    "                        old_path = os.path.join(root, file)\n",
    "                        new_path = os.path.join(root,'references.txt')\n",
    "                        os.rename(old_path,new_path)\n",
    "                        file_path = new_path\n",
    "                    elif 'references.txt' not in file:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                    else:\n",
    "                        continue\n",
    "                    zipf.write(file_path, file_path[len_dir_path:])\n",
    "                \n",
    "    list_of_files.append(f\"{zip_path}.zip\")\n",
    "    list_of_files.append(f\"{sub_root}.eeg\")\n",
    "    list_of_files.append(f\"{sub_root}.vhdr\")\n",
    "    list_of_files.append(f\"{sub_root}.vmrk\")\n",
    "    \n",
    "    return list_of_files\n",
    "\n",
    "#\n",
    "def createBidsDataset(the_bids_sample,bids_files,events_dict):\n",
    "    \"\"\"Create and save a new dataset from a parent sample and given files\"\"\"\n",
    "    \n",
    "    # create dataset for bids files\n",
    "    the_dset = o.new_dataset(type = 'RAWD_BIDS', \n",
    "                             sample = the_bids_sample,\n",
    "                             files = bids_files,\n",
    "                             props = {f'rawd_bids.name':events_dict['task_name'],\n",
    "                                      f'rawd_bids.session': events_dict['session_id'],\n",
    "                                      f'rawd_bids.phase': events_dict['project_phase'],\n",
    "                                      f'rawd_bids.participant': the_bids_sample.props('data_recording_mode.participant'),\n",
    "                                     },\n",
    "                            )\n",
    "    the_dset.save()\n",
    "#\n",
    "def getRawFilesAndData(raw_sample,the_stim_sample,download_path):\n",
    "    \"\"\"Get raw and stim files from database and download them locally.\"\"\"\n",
    "    \n",
    "    files_data = {}\n",
    "    \n",
    "    # get raw datasets to format\n",
    "    _datasets_raw = raw_sample.get_datasets()[0] # there should be only one dataset per session\n",
    "    _datasets_stim = the_stim_sample.get_datasets()[0] # idem\n",
    "\n",
    "    print('_datasets_raw.file_list: ',_datasets_raw.file_list)\n",
    "    _raw_signal = [_f for _f in _datasets_raw.file_list if '.json' in _f][0]\n",
    "    _raw_eeg = [_f for _f in _datasets_raw.file_list if 'EEG.csv' in _f][0]\n",
    "    _raw_eeg_layout = [_f for _f in _datasets_raw.file_list if 'Layout' in _f][0]\n",
    "    _raw_stim = [_f for _f in _datasets_stim.file_list if '.csv' in _f][0]\n",
    "    \n",
    "    # download locally the json,data file and read\n",
    "    _datasets_raw.download(f\"{_raw_signal}\",download_path)\n",
    "    _datasets_raw.download(f\"{_raw_eeg}\",download_path)\n",
    "    _datasets_raw.download(f\"{_raw_eeg_layout}\",download_path)\n",
    "    _datasets_stim.download(f\"{_raw_stim}\",download_path)\n",
    "    \n",
    "    signal_info_file = f\"{download_path}/{_datasets_raw.code}/original/DEFAULT/{_raw_signal.split('/')[-1]}\"\n",
    "    signal_info = readSamplingFrequencyFromRawData(signal_info_file, 'EEG')\n",
    "    stim_file = f\"{download_path}/{_datasets_stim.code}/original/DEFAULT/{_raw_stim.split('/')[-1]}\"\n",
    "    eeg_file = f\"{download_path}/{_datasets_raw.code}/original/DEFAULT/{_raw_eeg.split('/')[-1]}\"\n",
    "    eeg_layout = f\"{download_path}/{_datasets_raw.code}/original/DEFAULT/{_raw_eeg_layout.split('/')[-1]}\"\n",
    "    \n",
    "    files_data['sampling_frequency'] = signal_info\n",
    "    files_data['stim_file'] = stim_file\n",
    "    files_data['eeg_file'] = eeg_file\n",
    "    files_data['eeg_layout_file'] = eeg_layout\n",
    "    files_data['signals_file'] = signal_info_file\n",
    "    \n",
    "    return files_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56e997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for raw data mode: BLED\n",
      "get_samples posting request\n",
      "get_samples got response. Delay: 0.08200335502624512\n",
      "get_samples got JSON. Delay: 0.014375686645507812\n",
      "get_samples after result mapping. Delay: 0.0004973411560058594\n",
      "_sample_list_for_response before parsing JSON\n",
      "_sample_list_for_response got response. Delay: 0.0004143714904785156\n",
      "_sample_list_for_response computing result.\n",
      "_sample_list_for_response computed result. Delay: 0.00043773651123046875\n",
      "get_samples computed final result. Delay: 0.0021965503692626953\n",
      "   - found corresponding stimulation sample: NRMD_RAWD_STIM_3807_P001_S025\n",
      "   - getting existing data recording mode: NRMD_RAWD_BIDS_3807_P001_S025\n",
      "   - raw data download path: ./BIDS/3807_P001_S025\n",
      "_datasets_raw.file_list:  ['original/DEFAULT/signalsInfo.json', 'original/DEFAULT/signalsInfo.csv', 'original/DEFAULT/Photodiode.csv', 'original/DEFAULT/Layout.xlsx', 'original/DEFAULT/IMU_B.csv', 'original/DEFAULT/ExG_B.csv', 'original/DEFAULT/EEG.csv', 'original/DEFAULT/EEG-impedances.csv', 'original/DEFAULT/D in.csv']\n",
      "Files downloaded to: ./BIDS/3807_P001_S025/20230119165447100-1877\n",
      "Files downloaded to: ./BIDS/3807_P001_S025/20230119165447100-1877\n",
      "Files downloaded to: ./BIDS/3807_P001_S025/20230119165447100-1877\n",
      "Files downloaded to: ./BIDS/3807_P001_S025/20230118142625663-1741\n",
      "   - events dict: {'save_path': './BIDS', 'stim_csv_file': './BIDS/3807_P001_S025/20230118142625663-1741/original/DEFAULT/03807_P001_S025_T001_rsvp_paradigm_2022_Dec_15_1242.csv', 'signals_file': './BIDS/3807_P001_S025/20230119165447100-1877/original/DEFAULT/signalsInfo.json', 'raw_eeg_file': './BIDS/3807_P001_S025/20230119165447100-1877/original/DEFAULT/EEG.csv', 'eeg_layout_file': './BIDS/3807_P001_S025/20230119165447100-1877/original/DEFAULT/Layout.xlsx', 'participant_id': '3807', 'project_phase': 'P001', 'session_id': 'S025', 'task_name': 'T001', 'project_infos': {'proj.name': 'Neuramod', 'proj.funding': 'SNSF', 'proj.grant': '105213_192500', 'proj.institution': 'ETH Zurich', 'proj.authors': 'Pierre Cutellic, Nauman Khalid Qureshi'}, 'acquisition_infos': {'sensor.manufacturer': 'Bitbrain', 'sensor.product': 'Versatile EEG 32', 'sensor.modality': 'EEGD', 'sensor.type': 'Wet', 'sensor.channels': '32', 'sensor.units': 'µV', 'sensor.block_samples': '8', 'sensor.sampling_frequency': '256', 'sensor.power_line_frequency': '50', 'sensor.low_pass': None, 'sensor.high_pass': None, 'sensor.layout': 'standard_1020', 'sensor.reference': 'A2', 'sensor.ground': 'Afz'}, 'stim_infos': {'stim.phase': 'P001', 'stim.session': None, 'stim.paradigm': 'RSVP', 'stim.num': '5', 'stim.description': '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<html><head></head><body><p>Visual discrimination of a vertical black and white pattern image based on foucault gratings. Frequency is 1/11 with a square root smoothing (no smoothing). Standard rotations are applied for creating distractors. Tokens are reshuffled at every repetition within trials. Labels distribution is as follows: 1x \\'Target\\' (0 deg), 4x \\'Non-Target\\' (90 deg), 10x \\'Distractors\\' (10 deg, 13 deg, 14 deg, 15 deg, 28 deg, 31 deg, 34 deg, 36 deg, 40 deg, 43 deg).</p></body></html>', 'stim.display_time': '0.3', 'stim.notdisplay_time': '0.0', 'stim.rep_time': '15', 'stim.rest_time': '15.0', 'stim.trials_number': '5', 'stim.trial_start': '24.0', 'stim.datapoints_start': '300', 'stim.name': 'T001', 'stim.cog_task': 'ODDB', 'stim.label_name': 'rotation', 'stim.hardware': '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<html><head></head><body><p>Distance to Center Screen = 74cm \\xa0Display Resolution = 3840 x 2160 pixels Power Supply = AC 100 - 240 V ~ (+/- 10 %) Refresh Rate = 50/60 Hz &plusmn; 3 Hz</p></body></html>', 'stim.software': '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<html><head></head><body><p>Psychopy v2021.2.3</p></body></html>', 'stim.class_type': 'SUPD', 'stim.class_card': 'MULT'}}\n",
      "- stim.rep_time: 15\n",
      "- stim.num: 5\n",
      "- stim.rest_time: 15.0\n",
      "DEBUG x len: 604\n",
      "DEBUG events size: 1500\n",
      "DEBUG trial_number: 5\n",
      "DEBUG datapoints_start: 300\n",
      "DEBUG rest_time: 15.0\n",
      "DEBUG stimulus_display: 0.3\n",
      "DEBUG stimuli_number: 74\n",
      "DEBUG event_start: 6144\n",
      "DEBUG trial_length: 80391\n",
      "DEBUG trial_rest: 74708\n",
      "DEBUG sampling_frequency: 256\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (604) does not match length of index (1500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5847/1412786789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# all bids files to create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mbids_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateBidsFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticipant_par\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;31m# create dataset for bids files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mcreateBidsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_bids_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbids_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevents_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5847/475969082.py\u001b[0m in \u001b[0;36mcreateBidsFiles\u001b[0;34m(events_dict, the_participant)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# create an event file by task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     createEvents(save_path = events_dict['save_path'], \n\u001b[0m\u001b[1;32m    214\u001b[0m                  \u001b[0mdata_csv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stim_csv_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5847/475969082.py\u001b[0m in \u001b[0;36mcreateEvents\u001b[0;34m(save_path, data_csv_file, old_label, trial_number, sampling_frequency, trial_start, stimulus_display, datapoints_start, rest_time, stimuli_number, participant_id, project_phase, session_id, task_name)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event_sequence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event_sequence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'event_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trial'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant_installation/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant_installation/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
      "\u001b[0;32m/vagrant_installation/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant_installation/miniconda3/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (604) does not match length of index (1500)"
     ]
    }
   ],
   "source": [
    "the_bids_collection = o.get_collection('/MATERIALS/NRMD/NRMD_RAWD_BIDS')\n",
    "for mode in raw_modes:\n",
    "    print(f\"looking for raw data mode: {mode}\")\n",
    "    # get the main samples collection\n",
    "    the_raw_collection = o.get_collection(f'/MATERIALS/NRMD/NRMD_RAWD_{mode}')\n",
    "    the_raw_samples = the_raw_collection.get_samples()\n",
    "    # get the recording samples\n",
    "    for raw_sample in the_raw_samples:\n",
    "        \n",
    "        raw_parents = o.get_object(raw_sample.permId).parents\n",
    "        #print(sample.code)\n",
    "        sample_code = raw_sample.code.split('_')\n",
    "        '''\n",
    "        try:\n",
    "        '''\n",
    "\n",
    "        # check if the session is a valid one and not one of SXXX, SYYY, ...\n",
    "        try:\n",
    "            int(sample_code[5][1:])\n",
    "        except  Exception as e:\n",
    "            print(f\"   - session invalid: {e}\")\n",
    "            continue\n",
    "\n",
    "        # get or create the raw bids samples collection\n",
    "        try:\n",
    "            the_stim_sample = o.get_sample(f'/MATERIALS/NRMD/NRMD_RAWD_STIM_{sample_code[3]}_{sample_code[4]}_{sample_code[5]}')\n",
    "            print(f\"   - found corresponding stimulation sample: {the_stim_sample.code}\")\n",
    "        except  Exception as e:\n",
    "            print(f\"   - stimulation sample {the_stim_sample.code} not found. Skipping...\")\n",
    "            print(f\"   - error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            the_bids_sample = o.get_sample(f'/MATERIALS/NRMD/NRMD_RAWD_BIDS_{sample_code[3]}_{sample_code[4]}_{sample_code[5]}')\n",
    "            print(f\"   - getting existing data recording mode: {the_bids_sample.code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            the_bids_sample = o.new_sample(type = 'DATA_RECORDING_MODE',\n",
    "                                           code = f'NRMD_RAWD_BIDS_{sample_code[3]}_{sample_code[4]}_{sample_code[5]}',\n",
    "                                           project = raw_sample.project,\n",
    "                                           space = 'MATERIALS',\n",
    "                                           collection = f'/MATERIALS/NRMD/{the_bids_collection.code}',\n",
    "                                           props = {'data_recording_mode.data_recording_mode':raw_sample.props()['data_recording_mode.data_recording_mode'],\n",
    "                                                    'data_recording_mode.session': raw_sample.props()['data_recording_mode.session'],\n",
    "                                                    'data_recording_mode.phase': raw_sample.props()['data_recording_mode.phase'],\n",
    "                                                    'data_recording_mode.participant': raw_sample.props()['data_recording_mode.participant'],\n",
    "                                                   },\n",
    "                                            )\n",
    "            the_bids_sample.save()\n",
    "            the_bids_sample.parents = raw_parents + [raw_sample,the_stim_sample]\n",
    "            the_bids_sample.save()\n",
    "            print(f\"   - getting newly created data recording mode: {the_bids_sample.code}\")\n",
    "\n",
    "\n",
    "        download_path = os.path.join(bids_path,f\"{sample_code[3]}_{sample_code[4]}_{sample_code[5]}\")\n",
    "        proj_par = [_p for _p in raw_parents if o.get_object(_p).type == 'PROJECT'][0]\n",
    "        sensor_par = [_p for _p in raw_parents if o.get_object(_p).type == 'SENSOR'][0]\n",
    "        stim_par = [_p for _p in raw_parents if o.get_object(_p).type == 'STIM'][0]\n",
    "        participant_par = [_p for _p in raw_parents if o.get_object(_p).type == 'PARTICIPANT'][0]\n",
    "        print(f\"   - raw data download path: {download_path}\")\n",
    "        \n",
    "        # check if bids dataset already exists for the task\n",
    "        if len(list(the_bids_sample.get_datasets())) == 0:\n",
    "            # get raw datasets to format\n",
    "            try:\n",
    "                infos = getRawFilesAndData(raw_sample,the_stim_sample,download_path)\n",
    "            except Exception as e:\n",
    "                print(f\"   - raw data error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # create bids files by task\n",
    "            events_dict = {'save_path': bids_path,\n",
    "                           'stim_csv_file': infos['stim_file'],\n",
    "                           'signals_file': infos['signals_file'],\n",
    "                           'raw_eeg_file': infos['eeg_file'],\n",
    "                           'eeg_layout_file': infos['eeg_layout_file'],\n",
    "                           'participant_id': sample_code[3],\n",
    "                           'project_phase': sample_code[4],\n",
    "                           'session_id': sample_code[5],\n",
    "                           'task_name': o.get_object(stim_par).props('stim.name'),\n",
    "                           'project_infos': o.get_object(proj_par).props(),\n",
    "                           'acquisition_infos': o.get_object(sensor_par).props(), \n",
    "                           'stim_infos': o.get_object(stim_par).props()\n",
    "                          }\n",
    "            print(f\"   - events dict: {events_dict}\")\n",
    "            #if the_bids_sample.get_datasets()[0].props('rawd_bids.name')!= events_dict['task_name']:\n",
    "\n",
    "            # all bids files to create the dataset\n",
    "            bids_files = createBidsFiles(events_dict,participant_par)\n",
    "            # create dataset for bids files\n",
    "            createBidsDataset(the_bids_sample,bids_files,events_dict)\n",
    "            # remove downloaded files\n",
    "            shutil.rmtree(bids_path) #bids_path\n",
    "            print(f\"   - files removed from: {download_path}\")\n",
    "            \n",
    "            #else:\n",
    "            #    print(f\"   - task dataset already exists. Skipping...\")\n",
    "        else: \n",
    "            print(f\"   - task dataset already exists. Skipping...\")\n",
    "        '''\n",
    "        except Exception as e:\n",
    "            print (f\"Error: {e}\")\n",
    "            print(f\"   - session {raw_sample.code} not valid. Skipping...\")\n",
    "        '''\n",
    "\n",
    "print(\"DONE\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedb69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
